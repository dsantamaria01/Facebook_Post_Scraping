{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f010b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from selenium.webdriver.common.keys import Keys # \n",
    "import os\n",
    "\n",
    "\n",
    "###########################  FACEBOOK  #########################################\n",
    "#################################################################################   \n",
    "file = open('UPWORK_INFO.txt')\n",
    "all_lines = file.readlines()\n",
    "google_key = all_lines[1].split('\\n', 1)[0] \n",
    "\n",
    "Facebook_user = all_lines[4].split('\\n', 1)[0] \n",
    "Facebook_password = all_lines[7].split('\\n', 1)[0]  \n",
    "code_german = google_key\n",
    "\n",
    "import gspread\n",
    "\n",
    "sa = gspread.service_account(filename = 'connection-sheets-363309-78f2fe3fd434.json')\n",
    "sh = sa.open_by_key(code_german)\n",
    "wks = sh.worksheet('Foglio1')\n",
    "\n",
    "import pandas as pd \n",
    "all_data = pd.DataFrame(wks.get_all_records())\n",
    "facebook_data = all_data[(all_data['Platform'].str.contains('Facebook'))==True]\n",
    "cycles = len(facebook_data['Platform'])\n",
    "\n",
    "for index, row in facebook_data.iterrows():\n",
    "    #print(row['Unique Post URL'])\n",
    "    page = row['Unique Post URL']\n",
    "    \n",
    "    \n",
    "    driver = webdriver.Edge()\n",
    "    driver.get(page)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    time.sleep(3)\n",
    "    try: \n",
    "        driver.find_elements(By.XPATH,\".//*[@class='x1lliihq x6ikm8r x10wlt62 x1n2onr6 xlyipyv xuxw1ft']\")[6].click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #driver.find_elements(By.XPATH,\".//*[@class='x1lliihq x6ikm8r x10wlt62 x1n2onr6 xlyipyv xuxw1ft']\")[0].click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    driver.find_element(By.XPATH,\"//*[@name='email']\").send_keys(Facebook_user)\n",
    "    driver.find_element(By.XPATH,\"//*[@name='pass']\").send_keys(Facebook_password)\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH,\"//*[@aria-label='Accessible login button']\").click()\n",
    "    time.sleep(4)\n",
    "    driver.find_element(By.XPATH,\"//*[@id='facebook']\").click()\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "    LIKES=[]\n",
    "    try:\n",
    "        LIKES = driver.find_element(By.XPATH,\".//*[@class='x16hj40l']\").text \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if LIKES != []:\n",
    "\n",
    "        LIKES = driver.find_element(By.XPATH,\".//*[@class='x16hj40l']\").text \n",
    "\n",
    "        num_comm = 0\n",
    "        comment_raw  =0\n",
    "        try:\n",
    "            comment_raw = driver.find_element(By.XPATH,\".//*[@class='x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xi81zsa']\").text \n",
    "            lon = comment_raw.find(' ')\n",
    "            if lon != -1:\n",
    "                num_comm = comment_raw[:lon]  \n",
    "            else:\n",
    "                num_comm = 'no hay comentarios'\n",
    "        except:\n",
    "            num_comm = 'no hay comentarios'  \n",
    "        #print(num_comm,comment_raw)\n",
    "\n",
    "\n",
    "        num_share = 0\n",
    "        share_raw= 0\n",
    "        try:\n",
    "            share_raw = driver.find_elements(By.XPATH,\"//*[@class='x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xi81zsa']\")[1].text \n",
    "            lon = share_raw.find(' ')\n",
    "            if lon != -1:\n",
    "                num_share = share_raw[:lon]  \n",
    "            else:\n",
    "                num_share = '0'\n",
    "        except:\n",
    "            num_share = '0'\n",
    "        #print(num_share,share_raw )\n",
    "\n",
    "\n",
    "        SCROLL_PAUSE_TIME = 0.5\n",
    "        body = driver.find_element(by=By.TAG_NAME, value='body')\n",
    "        # Get scroll height\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        usuarios_face =[]\n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            #body.send_keys(Keys.PAGE_DOWN)\n",
    "            driver.find_element(by=By.TAG_NAME, value='body').send_keys(Keys.END)\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "            face_x = driver.find_elements(By.XPATH,\".//*[@class='x1i10hfl xjbqb8w x6umtig x1b1mbwd xaqea5y xav7gou x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz x1heor9g xt0b8zv']\")\n",
    "            for n in face_x:\n",
    "                try:\n",
    "                    usuarios_face.append(n.get_attribute(\"href\"))\n",
    "                except:\n",
    "                    pass   \n",
    "\n",
    "\n",
    "            try:\n",
    "                 driver.find_element(By.XPATH,\"//*[@class='x78zum5 x1w0mnb xeuugli']\").click() # more page\n",
    "            except:\n",
    "                pass \n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "\n",
    "        url_face = list(dict.fromkeys(usuarios_face))\n",
    "        delim = '         ,' \n",
    "        URLS = delim.join(url_face[2:])\n",
    "\n",
    "        wks.update_acell('D'+str(index+2), LIKES )\n",
    "        wks.update_acell('E'+str(index+2), num_comm )\n",
    "        wks.update_acell('F'+str(index+2), num_share )\n",
    "        wks.update_acell('H'+str(index+2), URLS)\n",
    "\n",
    "        driver.close()\n",
    "    else  :\n",
    "         #print('bad request verify URL  ', index+2 )\n",
    "        wks.update_acell('D'+str(index+2), 'bad request verify URL  ' )   \n",
    "        driver.close()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#########################################################################################\n",
    "#########################################################################################\n",
    "###########################            REDDIT                                   #####################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "code_german = google_key\n",
    "\n",
    "import gspread\n",
    "\n",
    "sa = gspread.service_account(filename = 'connection-sheets-363309-78f2fe3fd434.json')\n",
    "sh = sa.open_by_key(code_german)\n",
    "wks = sh.worksheet('Foglio1')\n",
    "\n",
    "import pandas as pd \n",
    "all_data = pd.DataFrame(wks.get_all_records())\n",
    "reddit_data = all_data[(all_data['Platform'].str.contains('Reddit'))==True]\n",
    "cycles = len(reddit_data['Platform'])\n",
    "\n",
    "\n",
    "for index, row in reddit_data.iterrows():\n",
    "    #print(row['Unique Post URL'])\n",
    "    page = row['Unique Post URL']\n",
    "\n",
    "\n",
    "    driver = webdriver.Edge()\n",
    "    driver.get(page)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    time.sleep(3)\n",
    "    likes_reddit = []\n",
    "    try:\n",
    "        likes_reddit = driver.find_elements(By.XPATH,\".//*[@class='_1rZYMD_4xY3gRcSS3p8ODO _3a2ZHWaih05DgAOtvu6cIo ']\")[0].text\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "    if likes_reddit != []:\n",
    "    \n",
    "        Comments_raw = driver.find_elements(By.XPATH,\".//*[@class='FHCV02u6Cp2zYL0fhQPsO']\")[0].text \n",
    "        lon = Comments_raw.find(\" \")\n",
    "        Comments_reddit = Comments_raw[:lon]\n",
    "        #Comments_reddit\n",
    "\n",
    "        old_position = 0\n",
    "        new_position = None\n",
    "        usuarios_r = []\n",
    "        body = driver.find_element(by=By.TAG_NAME, value='body')\n",
    "        while new_position != old_position:\n",
    "            # Get old scroll position\n",
    "            old_position = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            #print('o  '+ str(old_position))\n",
    "            reddits_x = driver.find_elements(By.XPATH,\"//*[@data-testid='comment_author_link']\")\n",
    "\n",
    "            for n in reddits_x:\n",
    "                try:\n",
    "                    usuarios_r.append(n.get_attribute(\"href\"))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Sleep and Scroll\n",
    "            time.sleep(1)\n",
    "            body.send_keys(Keys.PAGE_DOWN)\n",
    "            #driver.find_element(by=By.TAG_NAME, value='body').send_keys(Keys.END)\n",
    "\n",
    "            # Get new position\n",
    "            new_position = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            #print('n  '+ str(new_position))\n",
    "\n",
    "            reddits_x = driver.find_elements(By.XPATH,\"//*[@data-testid='comment_author_link']\")\n",
    "\n",
    "            for n in reddits_x:\n",
    "                try:\n",
    "                    usuarios_r.append(n.get_attribute(\"href\"))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Sleep and Scroll\n",
    "            time.sleep(1)\n",
    "            body.send_keys(Keys.PAGE_DOWN)\n",
    "            #driver.find_element(by=By.TAG_NAME, value='body').send_keys(Keys.END)\n",
    "\n",
    "\n",
    "        url_r = list(dict.fromkeys(usuarios_r))\n",
    "\n",
    "        delim = '         ,' \n",
    "        URLS_r = delim.join(url_r)\n",
    "\n",
    "        wks.update_acell('D'+str(index+2), likes_reddit )\n",
    "        wks.update_acell('E'+str(index+2), Comments_reddit)\n",
    "        #wks.update_acell('F'+str(index+2), likes_reddit)\n",
    "        wks.update_acell('H'+str(index+2), URLS_r)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        driver.close()\n",
    "    else:\n",
    "        #print('bad request verify URL  ', index+2 )\n",
    "        wks.update_acell('D'+str(index+2), 'Hmm...this page doesn’t exist. Try searching for something else....  ' )   \n",
    "        driver.close()\n",
    "         \n",
    "    \n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "###########################            TWITTER                                   #####################\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "\n",
    "code_german =  google_key\n",
    "\n",
    "import gspread\n",
    "\n",
    "sa = gspread.service_account(filename = 'connection-sheets-363309-78f2fe3fd434.json')\n",
    "sh = sa.open_by_key(code_german)\n",
    "wks = sh.worksheet('Foglio1')\n",
    "\n",
    "import pandas as pd \n",
    "all_data = pd.DataFrame(wks.get_all_records())\n",
    "Twitter_data = all_data[(all_data['Platform'].str.contains('Twitter'))==True]\n",
    "cycles = len(Twitter_data['Platform'])\n",
    "\n",
    "for index, row in Twitter_data.iterrows():\n",
    "    #print(row['Unique Post URL'])\n",
    "    page = row['Unique Post URL']\n",
    "\n",
    "    driver = webdriver.Edge()\n",
    "    driver.get(page)\n",
    "    driver.maximize_window()\n",
    "\n",
    "\n",
    "    time.sleep(3)\n",
    "    try: \n",
    "        driver.find_elements(By.XPATH,\".//*[@class='css-901oao css-16my406 r-poiln3 r-bcqeeo r-qvutc0']\")[1].click()\n",
    "    except:\n",
    "        pass\n",
    "    try: \n",
    "        driver.find_elements(By.XPATH,\".//*[@class='css-901oao css-16my406 r-poiln3 r-bcqeeo r-qvutc0']\")[1].click()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    likes_raw  = []\n",
    "    try:\n",
    "         likes_raw = driver.find_elements(By.XPATH,\".//*[@class='css-1dbjc4n r-xoduu5 r-1udh08x']\")\n",
    "    except: \n",
    "        pass\n",
    "   \n",
    "    \n",
    "    if likes_raw  != []:\n",
    "    \n",
    "        retweets_t = likes_raw[0].text\n",
    "        shares_t = likes_raw[1].text\n",
    "        likes_t = likes_raw[2].text \n",
    "\n",
    "        usuarios =[]\n",
    "\n",
    "        body = driver.find_element(by=By.TAG_NAME, value='body')\n",
    "\n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            #body.send_keys(Keys.PAGE_DOWN)\n",
    "            driver.find_element(by=By.TAG_NAME, value='body').send_keys(Keys.END)\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "            tweets_x = driver.find_elements(By.XPATH,\"//*[@class='css-4rbku5 css-18t94o4 css-1dbjc4n r-1loqt21 r-1wbh5a2 r-dnmrzs r-1ny4l3l']\")\n",
    "            for n in tweets_x:\n",
    "                try:\n",
    "                    usuarios.append(n.get_attribute(\"href\"))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "            try:\n",
    "                driver.find_elements(By.XPATH,\"//*[@class='comments-comments-list__load-more-comments-button artdeco-button artdeco-button--muted artdeco-button--1 artdeco-button--tertiary ember-view']\")[0].click()\n",
    "            except:\n",
    "                pass \n",
    "\n",
    "            try:\n",
    "                driver.find_elements(By.XPATH,\"//*[@class='css-18t94o4 css-1dbjc4n r-1777fci r-1pl7oy7 r-1ny4l3l r-o7ynqc r-6416eg r-13qz1uu']\")[0].click()\n",
    "            except:\n",
    "                pass \n",
    "        \n",
    "           \n",
    "            try:\n",
    "                 driver.find_elements(By.XPATH,\"//*[@class='css-901oao css-16my406 css-1hf3ou5 r-poiln3 r-1b43r93 r-1cwl3u0 r-bcqeeo r-qvutc0']\")[0].click() \n",
    "            except:\n",
    "                pass  \n",
    "            \n",
    "            \n",
    "            time.sleep(2)\n",
    "\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "\n",
    "\n",
    "        url_t = list(dict.fromkeys(usuarios))  # removes duplicates\n",
    "\n",
    "\n",
    "        url_tt = []\n",
    "\n",
    "        txt = url_t\n",
    "        for n in url_t:\n",
    "            try:\n",
    "               # lon = n.find(\"/\", 20 )\n",
    "                #if lon != -1:\n",
    "                url_tt.append(n)\n",
    "                    #print(lon,url_tt )\n",
    "            except:\n",
    "                pass\n",
    "        url_tt = list(dict.fromkeys(url_tt))  # URLS\n",
    "\n",
    "        comment_t = len(url_tt)-3             # nuemro de comentarios\n",
    "\n",
    "\n",
    "        delim = '     ,' \n",
    "        URLS_t = delim.join(url_tt)\n",
    "\n",
    "        wks.update_acell('D'+str(index+2), likes_t)\n",
    "        wks.update_acell('E'+str(index+2), comment_t)\n",
    "        wks.update_acell('F'+str(index+2), shares_t)\n",
    "        wks.update_acell('G'+str(index+2), retweets_t)\n",
    "        wks.update_acell('H'+str(index+2), URLS_t)\n",
    "\n",
    "        driver.close()\n",
    "        \n",
    "    else:\n",
    "        #print('bad request verify URL  ', index+2 )\n",
    "        wks.update_acell('D'+str(index+2), 'Hmm...this page doesn’t exist. Try searching for something else....  ' )   \n",
    "        driver.close()\n",
    "        \n",
    "        \n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "  ######################################    LINKEDIN ############################################\n",
    "#######################################################################################################\n",
    "\n",
    "username_N = all_lines[10].split('\\n', 1)[0]\n",
    "password_N = all_lines[13].split('\\n', 1)[0]  \n",
    "code_german = google_key\n",
    "\n",
    "code_german = google_key\n",
    "\n",
    "import gspread\n",
    "\n",
    "sa = gspread.service_account(filename = 'connection-sheets-363309-78f2fe3fd434.json')\n",
    "sh = sa.open_by_key(code_german)\n",
    "wks = sh.worksheet('Foglio1')\n",
    "\n",
    "import pandas as pd \n",
    "all_data = pd.DataFrame(wks.get_all_records())\n",
    "Linkedin_data = all_data[(all_data['Platform'].str.contains('Linkedin'))==True]\n",
    "cycles = len(Linkedin_data['Platform'])\n",
    "\n",
    "\n",
    "\n",
    "for index, row in Linkedin_data.iterrows():\n",
    "    #print(row['Unique Post URL'])\n",
    "    page = row['Unique Post URL']\n",
    "\n",
    "    driver = webdriver.Edge()\n",
    "    driver.get(page)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    try: \n",
    "        driver.find_elements(By.XPATH,\".//*[@class='artdeco-global-alert-action artdeco-button artdeco-button--inverse artdeco-button--2 artdeco-button--primary']\")[1].click()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    likes_linkedin = []\n",
    "    try: \n",
    "        likes_linkedin = driver.find_elements(By.XPATH,\".//*[@class='font-normal ml-0.5']\")[0].text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if likes_linkedin != []:\n",
    "        time.sleep(2)\n",
    "\n",
    "        try: \n",
    "            Comments_raw = driver.find_elements(By.XPATH,\".//*[@class='social-action-counts__social-counts-item social-action-counts__social-comments-counts']\")[0].text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try: \n",
    "            Comments_raw = driver.find_elements(By.XPATH,\".//*[@data-test-id='social-actions__comments']\")[0].text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        lon = Comments_raw.find(\" \")\n",
    "        Comments_linkedin = Comments_raw[:lon]\n",
    "        Comments_linkedin \n",
    "\n",
    "        driver.find_elements(By.XPATH,\"//*[@class='nav__button-secondary btn-md btn-secondary-emphasis']\")[0].click()\n",
    "\n",
    "\n",
    "\n",
    "        driver.find_element(By.XPATH,\"//*[@id='username']\").send_keys(username_N)\n",
    "        driver.find_element(By.XPATH,\"//*[@id='password']\").send_keys(password_N)\n",
    "\n",
    "        driver.find_elements(By.XPATH,\"//*[@class='btn__primary--large from__button--floating']\")[0].click()\n",
    "\n",
    "\n",
    "        usuarios_link =[]\n",
    "        SCROLL_PAUSE_TIME = 0.5\n",
    "        body = driver.find_element(by=By.TAG_NAME, value='body')\n",
    "\n",
    "        # Get scroll height\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            #body.send_keys(Keys.PAGE_DOWN)\n",
    "            driver.find_element(by=By.TAG_NAME, value='body').send_keys(Keys.END)\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "            linkedin_x = driver.find_elements(By.XPATH,\"//*[@class='app-aware-link  inline-flex overflow-hidden t-16 t-black t-bold tap-target']\")\n",
    "            for n in linkedin_x:\n",
    "                try:\n",
    "                    usuarios_link.append(n.get_attribute(\"href\"))\n",
    "                except:\n",
    "                    pass   \n",
    "\n",
    "\n",
    "            try:\n",
    "                 driver.find_elements(By.XPATH,\"//*[@class='comments-comments-list__load-more-comments-button artdeco-button artdeco-button--muted artdeco-button--1 artdeco-button--tertiary ember-view']\")[0].click()\n",
    "            except:\n",
    "                pass \n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "\n",
    "        url_link = list(dict.fromkeys(usuarios_link))  \n",
    "\n",
    "        delim = '  ,' \n",
    "        URLS_t = delim.join(url_link)\n",
    "\n",
    "\n",
    "        wks.update_acell('D'+str(index+2), likes_linkedin)\n",
    "        wks.update_acell('E'+str(index+2), Comments_linkedin )\n",
    "        #wks.update_acell('F'+str(index+2), shares_t)\n",
    "        #wks.update_acell('G'+str(index+2), retweets_t)\n",
    "        wks.update_acell('H'+str(index+2), URLS_t )\n",
    "\n",
    "        driver.close()\n",
    "    else:\n",
    "                #print('bad request verify URL  ', index+2 )\n",
    "        wks.update_acell('D'+str(index+2), 'Hmm...this page doesn’t exist. Try searching for something else....  ' )   \n",
    "        driver.close()\n",
    "    \n",
    "    \n",
    "  #####################################################################################################\n",
    "#######################################################################################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
